{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('wine.csv',header=None,usecols=[1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "labels=pd.read_csv('wine.csv',header=None,usecols=[0])\n",
    "data_list=data.values.tolist()\n",
    "labels_list=labels.values.tolist()\n",
    "data_array=np.array(data_list)\n",
    "labels_array=np.array(labels_list)\n",
    "print(data)\n",
    "print(labels)\n",
    "print(data_list)\n",
    "print(data_array)\n",
    "print(labels_array)\n",
    "print(data_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".values方法直接获得数组array，不用再转乘list之后再转换成array数组格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data_array, labels_array, test_size = 0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传入的是有label的数据，则用此方法\n",
    "其中train_data为训练输入的数据，train_target为数据类别，test_size为测试集占数据集的比例，random_state为随机数种子\n",
    "这里的random_state就是为了保证程序每次运行都分割一样的训练集和测试集。否则，同样的算法模型在不同的训练集和测试集上的效果不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1     2     3     4    5     6     7     8     9      10     11    12  \\\n",
      "61   12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62   5.75  0.980  1.59   \n",
      "51   13.83  1.65  2.60  17.2   94  2.45  2.99  0.22  2.29   5.60  1.240  3.37   \n",
      "66   13.11  1.01  1.70  15.0   78  2.98  3.18  0.26  2.28   5.30  1.120  3.18   \n",
      "37   13.05  1.65  2.55  18.0   98  2.45  2.43  0.29  1.44   4.25  1.120  2.51   \n",
      "4    13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.040  2.93   \n",
      "104  12.51  1.73  1.98  20.5   85  2.20  1.92  0.32  1.48   2.94  1.040  3.57   \n",
      "60   12.33  1.10  2.28  16.0  101  2.05  1.09  0.63  0.41   3.27  1.250  1.67   \n",
      "111  12.52  2.43  2.17  21.0   88  2.55  2.27  0.26  1.22   2.00  0.900  2.78   \n",
      "126  12.43  1.53  2.29  21.5   86  2.74  3.15  0.39  1.77   3.94  0.690  2.84   \n",
      "86   12.16  1.61  2.31  22.8   90  1.78  1.69  0.43  1.56   2.45  1.330  2.26   \n",
      "112  11.76  2.68  2.92  20.0  103  1.75  2.03  0.60  1.05   3.80  1.230  2.50   \n",
      "164  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03   9.58  0.700  1.68   \n",
      "26   13.39  1.77  2.62  16.1   93  2.85  2.94  0.34  1.45   4.80  0.920  3.22   \n",
      "56   14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03   6.38  0.940  3.31   \n",
      "129  12.04  4.30  2.38  22.0   80  2.10  1.75  0.42  1.35   2.60  0.790  2.57   \n",
      "45   14.21  4.04  2.44  18.9  111  2.85  2.65  0.30  1.25   5.24  0.870  3.33   \n",
      "8    14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.20  1.080  2.85   \n",
      "44   13.05  1.77  2.10  17.0  107  3.00  3.00  0.28  2.03   5.04  0.880  3.35   \n",
      "161  13.69  3.26  2.54  20.0  107  1.83  0.56  0.50  0.80   5.88  0.960  1.82   \n",
      "92   12.69  1.53  2.26  20.7   80  1.38  1.46  0.58  1.62   3.05  0.960  2.06   \n",
      "94   11.62  1.99  2.28  18.0   98  3.02  2.26  0.17  1.35   3.25  1.160  2.96   \n",
      "174  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.700  1.56   \n",
      "24   13.50  1.81  2.61  20.0   96  2.53  2.61  0.28  1.66   3.52  1.120  3.82   \n",
      "30   13.73  1.50  2.70  22.5  101  3.00  3.25  0.29  2.38   5.70  1.190  2.71   \n",
      "93   12.29  2.83  2.22  18.0   88  2.45  2.25  0.25  1.99   2.15  1.150  3.30   \n",
      "101  12.60  1.34  1.90  18.5   88  1.45  1.36  0.29  1.35   2.45  1.040  2.77   \n",
      "113  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44   3.08  1.100  2.31   \n",
      "19   13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.10  0.960  3.36   \n",
      "135  12.60  2.46  2.20  18.5   94  1.62  0.66  0.63  0.94   7.10  0.730  1.58   \n",
      "74   11.96  1.09  2.30  21.0  101  3.38  2.14  0.13  1.65   3.21  0.990  3.13   \n",
      "..     ...   ...   ...   ...  ...   ...   ...   ...   ...    ...    ...   ...   \n",
      "142  13.52  3.17  2.72  23.5   97  1.55  0.52  0.50  0.55   4.35  0.890  2.06   \n",
      "150  13.50  3.12  2.62  24.0  123  1.40  1.57  0.22  1.25   8.60  0.590  1.30   \n",
      "147  12.87  4.61  2.48  21.5   86  1.70  0.65  0.47  0.86   7.65  0.540  1.86   \n",
      "29   14.02  1.68  2.21  16.0   96  2.65  2.33  0.26  1.98   4.70  1.040  3.59   \n",
      "99   12.29  3.17  2.21  18.0   88  2.85  2.99  0.45  2.81   2.30  1.420  2.83   \n",
      "82   12.08  1.13  2.51  24.0   78  2.00  1.58  0.40  1.40   2.20  1.310  2.72   \n",
      "79   12.70  3.87  2.40  23.0  101  2.83  2.55  0.43  1.95   2.57  1.190  3.13   \n",
      "115  11.03  1.51  2.20  21.5   85  2.46  2.17  0.52  2.01   1.90  1.710  2.87   \n",
      "148  13.32  3.24  2.38  21.5   92  1.93  0.76  0.45  1.25   8.42  0.550  1.62   \n",
      "177  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.610  1.60   \n",
      "72   13.49  1.66  2.24  24.0   87  1.88  1.84  0.27  1.03   3.74  0.980  2.78   \n",
      "77   11.84  2.89  2.23  18.0  112  1.72  1.32  0.43  0.95   2.65  0.960  2.52   \n",
      "25   13.05  2.05  3.22  25.0  124  2.63  2.68  0.47  1.92   3.58  1.130  3.20   \n",
      "81   12.72  1.81  2.20  18.8   86  2.20  2.53  0.26  1.77   3.90  1.160  3.14   \n",
      "167  12.82  3.37  2.30  19.5   88  1.48  0.66  0.40  0.97  10.26  0.720  1.75   \n",
      "169  13.40  4.60  2.86  25.0  112  1.98  0.96  0.27  1.11   8.50  0.670  1.92   \n",
      "39   14.22  3.99  2.51  13.2  128  3.00  3.04  0.20  2.08   5.10  0.890  3.53   \n",
      "58   13.72  1.43  2.50  16.7  108  3.40  3.67  0.19  2.04   6.80  0.890  2.87   \n",
      "140  12.93  2.81  2.70  21.0   96  1.54  0.50  0.53  0.75   4.60  0.770  2.31   \n",
      "88   11.64  2.06  2.46  21.6   84  1.95  1.69  0.48  1.35   2.80  1.000  2.75   \n",
      "70   12.29  1.61  2.21  20.4  103  1.10  1.02  0.37  1.46   3.05  0.906  1.82   \n",
      "87   11.65  1.67  2.62  26.0   88  1.92  1.61  0.40  1.34   2.60  1.360  3.21   \n",
      "36   13.28  1.64  2.84  15.5  110  2.60  2.68  0.34  1.36   4.60  1.090  2.78   \n",
      "21   12.93  3.80  2.65  18.6  102  2.41  2.41  0.25  1.98   4.50  1.030  3.52   \n",
      "9    13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85   7.22  1.010  3.55   \n",
      "103  11.82  1.72  1.88  19.5   86  2.50  1.64  0.37  1.42   2.06  0.940  2.44   \n",
      "67   12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04   4.68  1.120  3.48   \n",
      "117  12.42  1.61  2.19  22.5  108  2.00  2.09  0.34  1.61   2.06  1.060  2.96   \n",
      "47   13.90  1.68  2.12  16.0  101  3.10  3.39  0.21  2.14   6.10  0.910  3.33   \n",
      "172  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24   9.70  0.620  1.71   \n",
      "\n",
      "       13  \n",
      "61    450  \n",
      "51   1265  \n",
      "66    502  \n",
      "37   1105  \n",
      "4     735  \n",
      "104   672  \n",
      "60    680  \n",
      "111   325  \n",
      "126   352  \n",
      "86    495  \n",
      "112   607  \n",
      "164   615  \n",
      "26   1195  \n",
      "56    970  \n",
      "129   580  \n",
      "45   1080  \n",
      "8    1045  \n",
      "44    885  \n",
      "161   680  \n",
      "92    495  \n",
      "94    345  \n",
      "174   750  \n",
      "24    845  \n",
      "30   1285  \n",
      "93    290  \n",
      "101   562  \n",
      "113   434  \n",
      "19    845  \n",
      "135   695  \n",
      "74    886  \n",
      "..    ...  \n",
      "142   520  \n",
      "150   500  \n",
      "147   625  \n",
      "29   1035  \n",
      "99    406  \n",
      "82    630  \n",
      "79    463  \n",
      "115   407  \n",
      "148   650  \n",
      "177   560  \n",
      "72    472  \n",
      "77    500  \n",
      "25    830  \n",
      "81    714  \n",
      "167   685  \n",
      "169   630  \n",
      "39    760  \n",
      "58   1285  \n",
      "140   600  \n",
      "88    680  \n",
      "70    870  \n",
      "87    562  \n",
      "36    880  \n",
      "21    770  \n",
      "9    1045  \n",
      "103   415  \n",
      "67    510  \n",
      "117   345  \n",
      "47    985  \n",
      "172   660  \n",
      "\n",
      "[160 rows x 13 columns]\n",
      "[[2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]]\n",
      "        1     2     3     4    5     6     7     8     9      10    11    12  \\\n",
      "54   13.74  1.67  2.25  16.4  118  2.60  2.90  0.21  1.62   5.85  0.92  3.20   \n",
      "151  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.80  0.48  1.47   \n",
      "63   12.37  1.13  2.16  19.0   87  3.50  3.10  0.19  1.87   4.45  1.22  2.87   \n",
      "55   13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45   6.25  0.98  3.03   \n",
      "123  13.05  5.80  2.13  21.5   86  2.62  2.65  0.30  2.01   2.60  0.73  3.10   \n",
      "121  11.56  2.05  3.23  28.5  119  3.18  5.08  0.47  1.87   6.00  0.93  3.69   \n",
      "7    14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25   5.05  1.06  3.58   \n",
      "160  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.65  0.56  1.58   \n",
      "106  12.25  1.73  2.12  19.0   80  1.65  2.03  0.37  1.63   3.40  1.00  3.17   \n",
      "90   12.08  1.83  2.32  18.5   81  1.60  1.50  0.52  1.64   2.40  1.08  2.27   \n",
      "141  13.36  2.56  2.35  20.0   89  1.40  0.50  0.37  0.64   5.60  0.70  2.47   \n",
      "146  13.88  5.04  2.23  20.0   80  0.98  0.34  0.40  0.68   4.90  0.58  1.33   \n",
      "5    14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.75  1.05  2.85   \n",
      "98   12.37  1.07  2.10  18.5   88  3.52  3.75  0.24  1.95   4.50  1.04  2.77   \n",
      "168  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.66  0.74  1.80   \n",
      "80   12.00  0.92  2.00  19.0   86  2.42  2.26  0.30  1.43   2.50  1.38  3.12   \n",
      "33   13.76  1.53  2.70  19.5  132  2.95  2.74  0.50  1.35   5.40  1.25  3.00   \n",
      "18   14.19  1.59  2.48  16.5  108  3.30  3.93  0.32  1.86   8.70  1.23  2.82   \n",
      "\n",
      "       13  \n",
      "54   1060  \n",
      "151   480  \n",
      "63    420  \n",
      "55   1120  \n",
      "123   380  \n",
      "121   465  \n",
      "7    1295  \n",
      "160   520  \n",
      "106   510  \n",
      "90    480  \n",
      "141   780  \n",
      "146   415  \n",
      "5    1450  \n",
      "98    660  \n",
      "168   750  \n",
      "80    278  \n",
      "33   1235  \n",
      "18   1680  \n",
      "[[1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(data_train)\n",
    "print(labels_train)\n",
    "print(data_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.264e+01 1.360e+00 2.020e+00 ... 9.800e-01 1.590e+00 4.500e+02]\n",
      " [1.383e+01 1.650e+00 2.600e+00 ... 1.240e+00 3.370e+00 1.265e+03]\n",
      " [1.311e+01 1.010e+00 1.700e+00 ... 1.120e+00 3.180e+00 5.020e+02]\n",
      " ...\n",
      " [1.242e+01 1.610e+00 2.190e+00 ... 1.060e+00 2.960e+00 3.450e+02]\n",
      " [1.390e+01 1.680e+00 2.120e+00 ... 9.100e-01 3.330e+00 9.850e+02]\n",
      " [1.416e+01 2.510e+00 2.480e+00 ... 6.200e-01 1.710e+00 6.600e+02]]\n"
     ]
    }
   ],
   "source": [
    "train_list=train.values.tolist()\n",
    "train_array=np.array(train_list)\n",
    "print(train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1     2     3     4    5     6     7     8     9      10     11    12  \\\n",
      "61   12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62   5.75  0.980  1.59   \n",
      "51   13.83  1.65  2.60  17.2   94  2.45  2.99  0.22  2.29   5.60  1.240  3.37   \n",
      "66   13.11  1.01  1.70  15.0   78  2.98  3.18  0.26  2.28   5.30  1.120  3.18   \n",
      "37   13.05  1.65  2.55  18.0   98  2.45  2.43  0.29  1.44   4.25  1.120  2.51   \n",
      "4    13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.040  2.93   \n",
      "104  12.51  1.73  1.98  20.5   85  2.20  1.92  0.32  1.48   2.94  1.040  3.57   \n",
      "60   12.33  1.10  2.28  16.0  101  2.05  1.09  0.63  0.41   3.27  1.250  1.67   \n",
      "111  12.52  2.43  2.17  21.0   88  2.55  2.27  0.26  1.22   2.00  0.900  2.78   \n",
      "126  12.43  1.53  2.29  21.5   86  2.74  3.15  0.39  1.77   3.94  0.690  2.84   \n",
      "86   12.16  1.61  2.31  22.8   90  1.78  1.69  0.43  1.56   2.45  1.330  2.26   \n",
      "112  11.76  2.68  2.92  20.0  103  1.75  2.03  0.60  1.05   3.80  1.230  2.50   \n",
      "164  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03   9.58  0.700  1.68   \n",
      "26   13.39  1.77  2.62  16.1   93  2.85  2.94  0.34  1.45   4.80  0.920  3.22   \n",
      "56   14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03   6.38  0.940  3.31   \n",
      "129  12.04  4.30  2.38  22.0   80  2.10  1.75  0.42  1.35   2.60  0.790  2.57   \n",
      "45   14.21  4.04  2.44  18.9  111  2.85  2.65  0.30  1.25   5.24  0.870  3.33   \n",
      "8    14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.20  1.080  2.85   \n",
      "44   13.05  1.77  2.10  17.0  107  3.00  3.00  0.28  2.03   5.04  0.880  3.35   \n",
      "161  13.69  3.26  2.54  20.0  107  1.83  0.56  0.50  0.80   5.88  0.960  1.82   \n",
      "92   12.69  1.53  2.26  20.7   80  1.38  1.46  0.58  1.62   3.05  0.960  2.06   \n",
      "94   11.62  1.99  2.28  18.0   98  3.02  2.26  0.17  1.35   3.25  1.160  2.96   \n",
      "174  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.700  1.56   \n",
      "24   13.50  1.81  2.61  20.0   96  2.53  2.61  0.28  1.66   3.52  1.120  3.82   \n",
      "30   13.73  1.50  2.70  22.5  101  3.00  3.25  0.29  2.38   5.70  1.190  2.71   \n",
      "93   12.29  2.83  2.22  18.0   88  2.45  2.25  0.25  1.99   2.15  1.150  3.30   \n",
      "101  12.60  1.34  1.90  18.5   88  1.45  1.36  0.29  1.35   2.45  1.040  2.77   \n",
      "113  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44   3.08  1.100  2.31   \n",
      "19   13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.10  0.960  3.36   \n",
      "135  12.60  2.46  2.20  18.5   94  1.62  0.66  0.63  0.94   7.10  0.730  1.58   \n",
      "74   11.96  1.09  2.30  21.0  101  3.38  2.14  0.13  1.65   3.21  0.990  3.13   \n",
      "..     ...   ...   ...   ...  ...   ...   ...   ...   ...    ...    ...   ...   \n",
      "142  13.52  3.17  2.72  23.5   97  1.55  0.52  0.50  0.55   4.35  0.890  2.06   \n",
      "150  13.50  3.12  2.62  24.0  123  1.40  1.57  0.22  1.25   8.60  0.590  1.30   \n",
      "147  12.87  4.61  2.48  21.5   86  1.70  0.65  0.47  0.86   7.65  0.540  1.86   \n",
      "29   14.02  1.68  2.21  16.0   96  2.65  2.33  0.26  1.98   4.70  1.040  3.59   \n",
      "99   12.29  3.17  2.21  18.0   88  2.85  2.99  0.45  2.81   2.30  1.420  2.83   \n",
      "82   12.08  1.13  2.51  24.0   78  2.00  1.58  0.40  1.40   2.20  1.310  2.72   \n",
      "79   12.70  3.87  2.40  23.0  101  2.83  2.55  0.43  1.95   2.57  1.190  3.13   \n",
      "115  11.03  1.51  2.20  21.5   85  2.46  2.17  0.52  2.01   1.90  1.710  2.87   \n",
      "148  13.32  3.24  2.38  21.5   92  1.93  0.76  0.45  1.25   8.42  0.550  1.62   \n",
      "177  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.610  1.60   \n",
      "72   13.49  1.66  2.24  24.0   87  1.88  1.84  0.27  1.03   3.74  0.980  2.78   \n",
      "77   11.84  2.89  2.23  18.0  112  1.72  1.32  0.43  0.95   2.65  0.960  2.52   \n",
      "25   13.05  2.05  3.22  25.0  124  2.63  2.68  0.47  1.92   3.58  1.130  3.20   \n",
      "81   12.72  1.81  2.20  18.8   86  2.20  2.53  0.26  1.77   3.90  1.160  3.14   \n",
      "167  12.82  3.37  2.30  19.5   88  1.48  0.66  0.40  0.97  10.26  0.720  1.75   \n",
      "169  13.40  4.60  2.86  25.0  112  1.98  0.96  0.27  1.11   8.50  0.670  1.92   \n",
      "39   14.22  3.99  2.51  13.2  128  3.00  3.04  0.20  2.08   5.10  0.890  3.53   \n",
      "58   13.72  1.43  2.50  16.7  108  3.40  3.67  0.19  2.04   6.80  0.890  2.87   \n",
      "140  12.93  2.81  2.70  21.0   96  1.54  0.50  0.53  0.75   4.60  0.770  2.31   \n",
      "88   11.64  2.06  2.46  21.6   84  1.95  1.69  0.48  1.35   2.80  1.000  2.75   \n",
      "70   12.29  1.61  2.21  20.4  103  1.10  1.02  0.37  1.46   3.05  0.906  1.82   \n",
      "87   11.65  1.67  2.62  26.0   88  1.92  1.61  0.40  1.34   2.60  1.360  3.21   \n",
      "36   13.28  1.64  2.84  15.5  110  2.60  2.68  0.34  1.36   4.60  1.090  2.78   \n",
      "21   12.93  3.80  2.65  18.6  102  2.41  2.41  0.25  1.98   4.50  1.030  3.52   \n",
      "9    13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85   7.22  1.010  3.55   \n",
      "103  11.82  1.72  1.88  19.5   86  2.50  1.64  0.37  1.42   2.06  0.940  2.44   \n",
      "67   12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04   4.68  1.120  3.48   \n",
      "117  12.42  1.61  2.19  22.5  108  2.00  2.09  0.34  1.61   2.06  1.060  2.96   \n",
      "47   13.90  1.68  2.12  16.0  101  3.10  3.39  0.21  2.14   6.10  0.910  3.33   \n",
      "172  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24   9.70  0.620  1.71   \n",
      "\n",
      "       13  \n",
      "61    450  \n",
      "51   1265  \n",
      "66    502  \n",
      "37   1105  \n",
      "4     735  \n",
      "104   672  \n",
      "60    680  \n",
      "111   325  \n",
      "126   352  \n",
      "86    495  \n",
      "112   607  \n",
      "164   615  \n",
      "26   1195  \n",
      "56    970  \n",
      "129   580  \n",
      "45   1080  \n",
      "8    1045  \n",
      "44    885  \n",
      "161   680  \n",
      "92    495  \n",
      "94    345  \n",
      "174   750  \n",
      "24    845  \n",
      "30   1285  \n",
      "93    290  \n",
      "101   562  \n",
      "113   434  \n",
      "19    845  \n",
      "135   695  \n",
      "74    886  \n",
      "..    ...  \n",
      "142   520  \n",
      "150   500  \n",
      "147   625  \n",
      "29   1035  \n",
      "99    406  \n",
      "82    630  \n",
      "79    463  \n",
      "115   407  \n",
      "148   650  \n",
      "177   560  \n",
      "72    472  \n",
      "77    500  \n",
      "25    830  \n",
      "81    714  \n",
      "167   685  \n",
      "169   630  \n",
      "39    760  \n",
      "58   1285  \n",
      "140   600  \n",
      "88    680  \n",
      "70    870  \n",
      "87    562  \n",
      "36    880  \n",
      "21    770  \n",
      "9    1045  \n",
      "103   415  \n",
      "67    510  \n",
      "117   345  \n",
      "47    985  \n",
      "172   660  \n",
      "\n",
      "[160 rows x 13 columns]\n",
      "[[2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data_train\n",
    "Y=labels_train\n",
    "print(X)\n",
    "print(Y)\n",
    "clf=svm.SVC(gamma='scale')\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1     2     3     4    5     6     7     8     9      10    11    12  \\\n",
      "54   13.74  1.67  2.25  16.4  118  2.60  2.90  0.21  1.62   5.85  0.92  3.20   \n",
      "151  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.80  0.48  1.47   \n",
      "63   12.37  1.13  2.16  19.0   87  3.50  3.10  0.19  1.87   4.45  1.22  2.87   \n",
      "55   13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45   6.25  0.98  3.03   \n",
      "123  13.05  5.80  2.13  21.5   86  2.62  2.65  0.30  2.01   2.60  0.73  3.10   \n",
      "121  11.56  2.05  3.23  28.5  119  3.18  5.08  0.47  1.87   6.00  0.93  3.69   \n",
      "7    14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25   5.05  1.06  3.58   \n",
      "160  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.65  0.56  1.58   \n",
      "106  12.25  1.73  2.12  19.0   80  1.65  2.03  0.37  1.63   3.40  1.00  3.17   \n",
      "90   12.08  1.83  2.32  18.5   81  1.60  1.50  0.52  1.64   2.40  1.08  2.27   \n",
      "141  13.36  2.56  2.35  20.0   89  1.40  0.50  0.37  0.64   5.60  0.70  2.47   \n",
      "146  13.88  5.04  2.23  20.0   80  0.98  0.34  0.40  0.68   4.90  0.58  1.33   \n",
      "5    14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.75  1.05  2.85   \n",
      "98   12.37  1.07  2.10  18.5   88  3.52  3.75  0.24  1.95   4.50  1.04  2.77   \n",
      "168  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.66  0.74  1.80   \n",
      "80   12.00  0.92  2.00  19.0   86  2.42  2.26  0.30  1.43   2.50  1.38  3.12   \n",
      "33   13.76  1.53  2.70  19.5  132  2.95  2.74  0.50  1.35   5.40  1.25  3.00   \n",
      "18   14.19  1.59  2.48  16.5  108  3.30  3.93  0.32  1.86   8.70  1.23  2.82   \n",
      "\n",
      "       13  \n",
      "54   1060  \n",
      "151   480  \n",
      "63    420  \n",
      "55   1120  \n",
      "123   380  \n",
      "121   465  \n",
      "7    1295  \n",
      "160   520  \n",
      "106   510  \n",
      "90    480  \n",
      "141   780  \n",
      "146   415  \n",
      "5    1450  \n",
      "98    660  \n",
      "168   750  \n",
      "80    278  \n",
      "33   1235  \n",
      "18   1680  \n"
     ]
    }
   ],
   "source": [
    "print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [2], [1], [2], [2], [1], [3], [2], [2], [3], [3], [1], [2], [3], [2], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "Y_true=labels_test.tolist()\n",
    "print(Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_pre=[1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1]\n",
    "Y_true=labels_test.tolist()\n",
    "accuracy_score(Y_true,Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [2], [1], [2], [2], [1], [3], [2], [2], [3], [3], [1], [2], [3], [2], [1], [1]]\n",
      "[1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_true)\n",
    "print(Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X=data_train\n",
    "Y=labels_train\n",
    "clf=tree.DecisionTreeClassifier()\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_pre=[1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1]\n",
    "Y_true=labels_test.tolist()\n",
    "accuracy_score(Y_true,Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.208e+01 1.330e+00 2.300e+00 ... 1.070e+00 3.210e+00 6.250e+02]\n",
      " [1.208e+01 1.830e+00 2.320e+00 ... 1.080e+00 2.270e+00 4.800e+02]\n",
      " [1.200e+01 1.510e+00 2.420e+00 ... 1.050e+00 2.650e+00 4.500e+02]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.216e+01 1.610e+00 2.310e+00 ... 1.330e+00 2.260e+00 4.950e+02]\n",
      " [1.165e+01 1.670e+00 2.620e+00 ... 1.360e+00 3.210e+00 5.620e+02]\n",
      " [1.164e+01 2.060e+00 2.460e+00 ... 1.000e+00 2.750e+00 6.800e+02]]\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.216e+01 1.610e+00 2.310e+00 ... 1.330e+00 2.260e+00 4.950e+02]\n",
      " [1.165e+01 1.670e+00 2.620e+00 ... 1.360e+00 3.210e+00 5.620e+02]\n",
      " [1.164e+01 2.060e+00 2.460e+00 ... 1.000e+00 2.750e+00 6.800e+02]]\n",
      "[[1.208e+01 1.330e+00 2.300e+00 ... 1.070e+00 3.210e+00 6.250e+02]\n",
      " [1.208e+01 1.830e+00 2.320e+00 ... 1.080e+00 2.270e+00 4.800e+02]\n",
      " [1.200e+01 1.510e+00 2.420e+00 ... 1.050e+00 2.650e+00 4.500e+02]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=2)\n",
    "for train_array,test_array in kf.split(data_array):\n",
    "    print(data_array[train_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.208e+01 1.330e+00 2.300e+00 ... 1.070e+00 3.210e+00 6.250e+02]\n",
      " [1.208e+01 1.830e+00 2.320e+00 ... 1.080e+00 2.270e+00 4.800e+02]\n",
      " [1.200e+01 1.510e+00 2.420e+00 ... 1.050e+00 2.650e+00 4.500e+02]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(data_array[test_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "StratifiedShuffleSplit(n_splits=10,test_size=None,train_size=None, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数 n_splits是将训练数据分成train/test对的组数，可根据需要进行设置，默认为10\n",
    "\n",
    "参数test_size和train_size是用来设置train/test对中train和test所占的比例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
